# -*- coding: utf-8 -*-
"""blazerunner.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ZEGbgREF6up3kXxsnw1F72MxiKYglGu

Video analysis for Running Gait

As a user, I want the system to detect the foot that is facing towards the camera in each frame so that the key events can be identified accurately.
Acceptance Criteria: The system identifies and labels the foot facing the camera.

As a user, I want the system to identify the Initial Contact phase in the video so that I can analyze the runner’s foot strike.
Acceptance Criteria: The system identifies the frame where the foot first makes contact with the ground.

As a user, I want the system to identify the Mid Stance phase in the video so that I can analyze the runner’s weight-bearing phase.
Acceptance Criteria: The system identifies the frame where the runner’s body is directly over the support foot.

As a user, I want the system to identify the Push Off phase in the video so that I can analyze the propulsion phase.
Acceptance Criteria: The system identifies the frame where the foot is pushing off the ground.

As a user, I want the system to add markers to the key frames so that the important points in the gait cycle are clearly visible.
Acceptance Criteria: The system overlays markers on the key points (e.g., joints, foot) in the key frames.

As a user, I want to view the key frames with markers in a sequence so that I can visually inspect the gait phases.
Acceptance Criteria: The system displays the key frames with markers in a clear and organized manner.
"""

!pip install mediapipe

import cv2
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow

uploaded = {}

def FrameCapture(path):

	vidObj = cv2.VideoCapture(path)
	count = 0
	success = 1

	while success and count<300:
		success, image = vidObj.read()
		cv2.imwrite("frame%d.jpg" % count, image)
		uploaded["frame%d.jpg" % count] = image
		cv2_imshow(image)
		count += 1

if __name__ == '__main__':
	FrameCapture("20240211_092524.mp4")

import cv2
from google.colab.patches import cv2_imshow
import math

DESIRED_HEIGHT = 480
DESIRED_WIDTH = 480
def resize_and_show(image):
  h, w = image.shape[:2]
  if h < w:
    img = cv2.resize(image, (DESIRED_WIDTH, math.floor(h/(w/DESIRED_WIDTH))))
  else:
    img = cv2.resize(image, (math.floor(w/(h/DESIRED_HEIGHT)), DESIRED_HEIGHT))
  cv2_imshow(img)

images = {name: cv2.imread(name) for name in uploaded.keys()}
for name, image in images.items():
  print(name)
  resize_and_show(image)

import mediapipe as mp
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

help(mp_pose.Pose)

min = 10000
max = 0
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:
  for name, image in images.items():
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    image_hight, image_width, _ = image.shape
    if not results.pose_landmarks:
      continue
    if (results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight) < min:
      min = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight
    if (results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight) > max:
      max = results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight
  print(min, max)
  floor = (min + max * 6)/7
  print(floor)

count = 0
max_count = 0
max_angle = 360
numlist = []

flag = True

with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:
  for name, image in images.items():
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    image_hight, image_width, _ = image.shape
    if not results.pose_landmarks:
      continue

    if (results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight) > floor and flag:
      flag = False
      print("***", count)
      numlist.append(count)
    if not flag and (results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight) > floor and (results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX].y * image_hight) > floor:
      side1 = math.sqrt((results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].y * image_hight - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight)**2+(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].x * image_width - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].x * image_width)**2)
      side2 = math.sqrt((results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].y * image_hight - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].y * image_hight)**2+(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].x * image_width - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].x * image_width)**2)
      side3 = math.sqrt((results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].y * image_hight)**2+(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].x * image_width - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].x * image_width)**2)
      angle = math.acos((side1**2 + side2**2 - side3**2)/(2*side1*side2))*180/np.pi
      if angle < max_angle:
        max_angle = angle
        max_count = count
    if (results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight) < floor and not flag:
      flag = True
      print(max_angle)
      max_angle = 360
      numlist.append(max_count)
      print(count)
      numlist.append(count)
    count += 1
print(numlist)

import numpy as np
count = 0
with mp_pose.Pose(
    static_image_mode=True, min_detection_confidence=0.5, model_complexity=2) as pose:
  for name, image in images.items():
    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

    image_hight, image_width, _ = image.shape
    if not results.pose_landmarks:
      continue
    print(
      f'left heel coordinates: ('
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HEEL].x * image_width}, '
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HEEL].y * image_hight})'
    )
    print(
      f'right heel coordinates: ('
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].x * image_width}, '
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight})'
    )
    # print(
    #   f'left foot index coordinates: ('
    #   f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_FOOT_INDEX].x * image_width}, '
    #   f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_FOOT_INDEX].y * image_hight})'
    # )
    # print(
    #   f'right foot index coordinates: ('
    #   f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX].x * image_width}, '
    #   f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_FOOT_INDEX].y * image_hight})'
    # )
    print(
      f'left knee coordinates: ('
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE].x * image_width}, '
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE].y * image_hight})'
    )
    print(
      f'right knee coordinates: ('
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].x * image_width}, '
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].y * image_hight})'
    )
    print(
      f'left hip coordinates: ('
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].x * image_width}, '
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP].y * image_hight})'
    )
    print(
      f'right knee coordinates: ('
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].x * image_width}, '
      f'{results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].y * image_hight})'
    )
    # ang1 = np.arctan((results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].y * image_hight - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight)/(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].x * image_width - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].x * image_width))*180/np.pi
    # print(ang1)
    # ang2 = np.arctan((results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].y * image_hight - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].y * image_hight)/(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].x * image_width - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].x * image_width))*180/np.pi
    # print(ang2)
    # print(abs(ang2)-ang1)

    side1 = math.sqrt((results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].y * image_hight - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight)**2+(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].x * image_width - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].x * image_width)**2)
    side2 = math.sqrt((results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].y * image_hight - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].y * image_hight)**2+(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].x * image_width - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE].x * image_width)**2)
    side3 = math.sqrt((results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].y * image_hight - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].y * image_hight)**2+(results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HEEL].x * image_width - results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP].x * image_width)**2)
    # print(side1)
    # print(side2)
    # print(side3)
    print(math.acos((side1**2 + side2**2 - side3**2)/(2*side1*side2))*180/np.pi)
    count+=1

    print(f'Pose landmarks of {name}:')
    annotated_image = image.copy()
    mp_drawing.draw_landmarks(
        annotated_image,
        results.pose_landmarks,
        mp_pose.POSE_CONNECTIONS,
        landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style())
    if count in numlist:
      resize_and_show(annotated_image)

while True:
    frame, body = tracker.next_frame()
    if frame is None: break
    landmark_coords = body.landmarks_worl
    frame = renderer.draw(frame, body)
    key = renderer.waitKey(delay=1)
    if key == 27 or key == ord('q'):
        break
renderer.exit()
tracker.exit()

"""Frame-wise Splitting: Use OpenCV to read the video and then loop through each frame to process them individually.

Key Points through BlazePose: Key points are ankle, knee and hip joints.

Identifying Key Frames and Markers: After detecting the key points using BlazePose, you need to identify the key frames - initial contact, mid-stance, push-off. Achieved through heuristic analysis or machine learning models.

Processing Key Frames: Pass key frames to different functions depending on the event detected. Coordinates of points can be obtained from markers and used to calculate strides and required angles.

Display.
"""